{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857e1b02-c3ae-425e-ba67-018ec821efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Farama-Foundation/Gymnasium.git@main\n",
      "  Cloning https://github.com/Farama-Foundation/Gymnasium.git (to revision main) to /tmp/pip-req-build-4ggi2gsu\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/Gymnasium.git /tmp/pip-req-build-4ggi2gsu\n",
      "  Resolved https://github.com/Farama-Foundation/Gymnasium.git to commit 443b1940f11087280663e884edea571f47f72413\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0rc1) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0rc1) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0rc1) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0rc1) (0.0.4)\n",
      "Building wheels for collected packages: gymnasium\n",
      "  Building wheel for gymnasium (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gymnasium: filename=gymnasium-1.0.0rc1-py3-none-any.whl size=935227 sha256=9e375aa6ee225e0c98325666b13c8cc3e33b7c1a546963c75e3f5727b1a7a2fc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g69cprli/wheels/92/23/59/a716cdb94ccb633277f76fbfe20cbc1bb4eeb6d301de917a7e\n",
      "Successfully built gymnasium\n",
      "Installing collected packages: gymnasium\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 0.29.1\n",
      "    Uninstalling gymnasium-0.29.1:\n",
      "      Successfully uninstalled gymnasium-0.29.1\n",
      "Successfully installed gymnasium-1.0.0rc1\n",
      "Requirement already satisfied: gymnasium[box2d] in /opt/conda/lib/python3.10/site-packages (1.0.0rc1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (2.5.2)\n",
      "Requirement already satisfied: swig==4.* in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (4.1.1.post1)\n",
      "Requirement already satisfied: moviepy in /opt/conda/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.10/site-packages (from moviepy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.29.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/conda/lib/python3.10/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from moviepy) (1.23.5)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.28.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (9.5.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.5.7)\n",
      "Requirement already satisfied: pysdl2 in /opt/conda/lib/python3.10/site-packages (0.9.16)\n",
      "Requirement already satisfied: pyvirtualdisplay in /opt/conda/lib/python3.10/site-packages (3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Farama-Foundation/Gymnasium.git@main\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install moviepy --upgrade\n",
    "!pip install pysdl2\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2be0a562-7361-46ce-9f2e-cd93608f7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import wrappers\n",
    "#import pyvirtualdisplay\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bbeab60-2aa9-4fbd-b3f2-153d6d5650c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:282: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/chgaw002/DeepLearning/video/car_racing folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False, render_mode=\"rgb_array\")\n",
    "env = wrappers.RecordVideo(env, 'video/car_racing', episode_trigger=lambda n: n%200==0, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7818d63-0400-45e3-8448-ef260afbd4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(0, 255, (96, 96, 3), uint8), 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ada1723-08cd-4ad9-9501-78c8943e4d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def render(env, img):\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f2f371-34aa-4f52-ba69-6a6d67433205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def __call__(self, observation):\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "    def update(self, *args):\n",
    "        # Do nothing\n",
    "        pass\n",
    "    \n",
    "    def init_game(self, observation):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "93b8a496-337e-4817-8d9e-bbdd1335d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(policy, episodes=2000, do_render = False, seed=100):\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    policy.init_game(observation)\n",
    "\n",
    "    if do_render:\n",
    "        plt.ion()\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(env.render())\n",
    "   \n",
    "    status = {}\n",
    "    episode = 0\n",
    "    status['steps'] = 0\n",
    "    status['episode_reward'] = 0\n",
    "    status['average_reward'] = None\n",
    "    total_reward = 0\n",
    "    \n",
    "    env.metadata['status'] = status\n",
    "\n",
    "    with tqdm(total=episodes) as pbar:\n",
    "        pbar.set_postfix(status)\n",
    "        while True:\n",
    "            try:\n",
    "                action = policy(observation)\n",
    "                observation, reward, terminated, truncated, info = env.step(action)\n",
    "                status['steps'] += 1\n",
    "                status['episode_reward'] += reward\n",
    "                if do_render:\n",
    "                    render(env, img)\n",
    "                policy.update(observation, reward, terminated, truncated, info, status)\n",
    "                pbar.set_postfix(status)\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    episode += 1\n",
    "                    if episode > pbar.total:\n",
    "                        break\n",
    "                    total_reward += status['episode_reward']\n",
    "                    if status['average_reward'] is None:\n",
    "                        status['average_reward'] = status['episode_reward']\n",
    "                    else:\n",
    "                        status['average_reward'] = 0.05 * status['episode_reward'] + (1 - 0.05) * status['average_reward']\n",
    "                    if status['average_reward'] > env.spec.reward_threshold:\n",
    "                        print(f\"Solved! Running reward is now {status['average_reward']} and \"\n",
    "                              f\"the last episode runs to {status['steps']} time steps!\")\n",
    "                        break\n",
    "\n",
    "                    pbar.set_postfix(status)\n",
    "                    pbar.update()\n",
    "                    status['steps'] = 0\n",
    "\n",
    "                    status['episode_reward'] = 0\n",
    "                    observation, info = env.reset()\n",
    "                    policy.init_game(observation)\n",
    "            except KeyboardInterrupt:\n",
    "                env.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b3cff-ecbe-48cf-be3d-dad52d9f9dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = RandomPolicy(env)\n",
    "play_game(policy, episodes=1, do_render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d9c8489-cc8a-441e-903c-cf2dc3e6f0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape : list=[3,96,96],\n",
    "                 out_channels1 : int=5,\n",
    "                 out_channels2 : int=5,\n",
    "                 kernel_size : int=4,\n",
    "                 stride : int=2,\n",
    "                 hidden_size : int=256,\n",
    "                 num_actions : int=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], \n",
    "                               out_channels=out_channels1, \n",
    "                               kernel_size=kernel_size, \n",
    "                               stride=stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels1, \n",
    "                               out_channels=out_channels2, \n",
    "                               kernel_size=kernel_size, \n",
    "                               stride=stride)\n",
    "        \n",
    "        # Determine the shape of self.conv2 output and pass it to linear1\n",
    "        dummy_input = torch.rand(1,*input_shape)\n",
    "        with torch.no_grad():\n",
    "            out_conv2_shape = torch.flatten(self.conv2(self.conv1(dummy_input))).shape[0]\n",
    "    \n",
    "        self.linear1 = nn.Linear(out_conv2_shape, hidden_size)\n",
    "        self.policy = nn.Sequential(nn.Linear(hidden_size, num_actions), nn.Softmax())\n",
    "        self.value = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Adjust tensor to have shape [batch, *image_shape]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        p = self.policy(x)\n",
    "        v = self.value(x)\n",
    "        return p, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "15768762-02a0-4c80-98bb-2f078cc0790c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "    \n",
    "class ACPolicy:\n",
    "    \n",
    "    def __init__(self, env, gamma=0.99, lr=5e-3):\n",
    "        # Two possible actions 0, 1       \n",
    "        self.net = ActorCriticNetwork(num_actions=env.action_space.n)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.best_reward = None\n",
    "\n",
    "        \n",
    "    def __call__(self, observation):\n",
    "        x = torch.tensor(observation).permute(2, 0, 1) / 255\n",
    "        probs, value = self.net(x)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.memory.append(SavedAction(m.log_prob(action), value))\n",
    "        self.last_observation = observation\n",
    "        \n",
    "        return action.item()\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        \n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "                \n",
    "            if self.best_reward is None:\n",
    "                self.best_reward = self.total_reward\n",
    "            elif self.total_reward > self.best_reward:\n",
    "                self.best_reward = self.total_reward\n",
    "                status['best'] = self.best_reward \n",
    "                self.save('best.pt')\n",
    "                \n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "            \n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                advantage = discounted_reward - mem.value.item() \n",
    "                #print(mem.value)\n",
    "                policy_losses.append(-(mem.log_prob * advantage))\n",
    "                value_losses.append(F.smooth_l1_loss(mem.value, discounted_reward.unsqueeze(0).unsqueeze(0)))\n",
    "               \n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss = torch.stack(policy_losses).sum()\n",
    "            value_loss = torch.stack(value_losses).sum()\n",
    "            loss = policy_loss + value_loss \n",
    "            loss.backward()    \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            status['policy_loss'] = str(policy_loss.item())\n",
    "            status['value_loss'] = str(value_loss.item())\n",
    "            \n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "    \n",
    "    \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'games': self.games,\n",
    "                    'model_state_dict': self.net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6e300eb1-13ba-43ec-91a5-ca0a9c71671a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1671442b81284af092fdbbca4431564f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "policy = ACPolicy(env)\n",
    "policy.load('model_2000.pt')\n",
    "play_game(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71593edb-f80f-4321-a7b5-7a5093fe26d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy.save('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc6c7b08-34f0-4b6b-838c-9b1cd43ae13d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466fd614c49848afa6f841e88da853b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False, render_mode=\"rgb_array\")\n",
    "env = wrappers.RecordVideo(env, 'video/car_racing', fps=30)\n",
    "policy = ACPolicy(env)\n",
    "policy.load('model_2000.pt')\n",
    "play_game(policy, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293bbcf5-21fe-488f-804c-7c7298f861ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
