{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secret-issue",
   "metadata": {},
   "source": [
    "## Training eines neuronalen Netzes zur Erkennung von Ziffern\n",
    "\n",
    "Bei Training der Snake-KI und beim Training des Chat-Bots haben wir neuronale Netze als \"Black Box\" verwendet. Heute wollen wir uns etwas genauer anschauen, wie das Training eines neuronalen Netzes funktioniert.\n",
    "\n",
    "Wenn man eine neue Programmiersprache lernt, beginnt man meistens mit einem *Hello World* – einem kleinen Programm, das die Meldung `Hello World!` ausgibt.\n",
    "Im Bereich der KI bzw. des maschinellen Lernens verwendet man gerne den *[MNIST Datensatz](https://de.wikipedia.org/wiki/MNIST-Datenbank)* als Beispiel verwendet.\n",
    "\n",
    "In diesem Praktikum geht es um folgende Punkte:\n",
    "\n",
    "**1. Wie trainiert man ein neuronales Netz?**<br>\n",
    "   Wir lernen, wie man\n",
    "   - Trainingsdaten lädt,\n",
    "   - ein neuronales Netz aufbaut,\n",
    "   - das neuronale Netz trainiert und\n",
    "   - die Qualität des Netzes misst.\n",
    "\n",
    "\n",
    "**2. Wovon hängt die Qualität der Erkennung ab?**<br>\n",
    "   Jenachdem, \n",
    "   - wie wir unser Netz bauen, \n",
    "   - wie viele Trainingsdaten wir verwenden und \n",
    "   - wie lange wir das Netz trainieren\n",
    "   sind die Ergebnisse besser oder schlechter. Dabei gibt es typische Probleme, die beim Training von KI-Modellen immer wieder auftauchen. Wie beim Fußball ist es \n",
    "   wichtig, als \"Trainer\" diese Probleme zu erkennen und zu wissen, was man dagegen tun kann.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-membrane",
   "metadata": {},
   "source": [
    "In diesem Praktikum verwenden wir ein *Jupyter Notebook*. Darin kann man Erklärungen direkt in das Programm einbauen. Die folgende *Code-Zelle* gibt `Hello World!` aus, wenn Du sie ausführst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-elder",
   "metadata": {},
   "source": [
    "Hier geht es mit dem Trainingsprogramm los. Die folgende Code-Zelle importiert die notwendigen Bibliotheken, die wir im folgenden benötigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-bahamas",
   "metadata": {},
   "source": [
    "### Trainingsdaten laden\n",
    "\n",
    "Jetzt laden wir die Trainingsdaten aus dem Internet und schauen uns ein paar Beispiele an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.FashionMNIST(root=\"\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_set = datasets.FashionMNIST(root=\"\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-japanese",
   "metadata": {},
   "source": [
    "#### \"Form\" der Trainingsdaten\n",
    "\n",
    "Wir schauen uns nun an, welche \"Form\" (*shape*) die Trainings- und Testdaten haben. Verstehst Du, was die drei Zahlen bedeuten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_set shape: \", train_set.data.shape)\n",
    "print(\"test_set shape: \", test_set.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, pin_memory=True, num_workers=5, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, pin_memory=True, num_workers=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-enterprise",
   "metadata": {},
   "source": [
    "#### Trainingsbeispiele ansehen\n",
    "\n",
    "Die folgende Zelle lädt die ersten Trainingsdaten und zeigt ein paar Beispiele an. Damit können wir kontrollieren, dass alles geklappt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    # Wir laden die erste Portion Daten\n",
    "    break;\n",
    "    \n",
    "figure = plt.figure()\n",
    "num_images = 18\n",
    "for i in range(num_images):\n",
    "    plt.subplot(3, 6, i+1, title=f\"{labels[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(images[i].cpu().numpy().squeeze(), cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-version",
   "metadata": {},
   "source": [
    "### Aufbau des neuronalen Netzes\n",
    "\n",
    "Hier bauen wir nun das neuronale Netz. Wir versuchen es zunächst mit einem ganz einfachen Netz:\n",
    "\n",
    "- Eine Eigabeschicht, die die $28 \\cdot 28$ Pixel auf eine **versteckte Schicht** abbildet.\n",
    "  Diese versteckte Schicht hat `hidden_size` Neuronen. Wir beginnen mit zwei Neuronen \n",
    "  (`hidden_size = 2`). \n",
    "- Die Eingabeschicht verwendet als **Aktivierungsfunktion** die \n",
    "  *[Rectified Linear Unit (ReLU)](https://de.wikipedia.org/wiki/Rectifier_(neuronale_Netzwerke))*\n",
    "- Eine **Ausgabeschicht**, die für jede mögliche Ziffer ein Neuron enthält.\n",
    "  Hier benötigen wir keine Aktivierungsfunktion – das am \"stärksten feuernde\" Neuron gewinnt am Ende\n",
    "  bzw. die Werte werden automatisch so normiert, dass sie zwischen $0$ und $1$ liegen und in der Summe \n",
    "  $1$ ergeben ([Softmax](https://de.wikipedia.org/wiki/Softmax-Funktion)).\n",
    "  \n",
    "Zur Definition des Netzes benötigen wir eine Funktion `__init__()` um die Daten zu definieren, und eine \n",
    "Funktion `forward()` die Rechenschritte des Netzes ausführt.\n",
    "Den ganzen mathematischen Rest erledigt die Bibliothek *PyTorch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.view(-1, 28*28)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b6acd-95c9-443b-9691-0596800958b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020ff63-4e51-4833-8e9d-511c35e7a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Max pooling layer\n",
    "        x = self.pool(x)\n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = x.view(-1, 64 * 12 * 12)\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # Output layer\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc6288-5f16-4939-bc07-a9021dee6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmelieCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AmelieCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128*3*3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.pool2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.pool3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-blond",
   "metadata": {},
   "source": [
    "### Neuronales Netz, Verlustfunktion und Optimierer anlegen\n",
    "\n",
    "Neben dem neuronalen Netz erzeugen wir jetzt \n",
    "- Die Verlustfunktion, die wir optimieren. Sie misst, ob unser Netz die richtige Ziffer \"vorschlägt\" \n",
    "  und sich dabei möglichst sicher ist. Das heißt\n",
    "  - die Ausgabe des Neurons der \"richtigen\" Ziffer soll möglichst hoch,\n",
    "  - die Ausgabe der anderen Neuronen möglichst klein sein.\n",
    "  Dafür verwendet man die sogenannte *[Kreuzentropie](https://de.wikipedia.org/wiki/Kreuzentropie)* \n",
    "  (engl. *Cross Entropy*).\n",
    "- Den Optimierer, der beim Training die Verlustfunktion optimiert. Wir verwenden `optim.SGD`, wobei   \n",
    "  *SGD* für *Stochastic Gradient Descent* steht. Anschaulich gesprochen verhält sich dieser Optimierer\n",
    "  wie ein Skifahrer im Nebel – er fährt einfach dalang, wo es bergab geht, und hofft, dass er heil \n",
    "  im Tal ankommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AmelieCNN()\n",
    "model = model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "training_loss = []\n",
    "testing_loss = []\n",
    "training_acc = []\n",
    "testing_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-election",
   "metadata": {},
   "source": [
    "Hier schauen wir uns noch einmal an, wie viele Parameter unser Modell hat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-hampton",
   "metadata": {},
   "source": [
    "### Training des Modells\n",
    "\n",
    "Jetzt trainieren wir das Modell mit unseren Trainingsdaten.\n",
    "Jeden Durchlauf über die kompletten Trainingsdaten nennt man *Epoche*. \n",
    "\n",
    "Da wir nur wenige Trainingsbilder verwenden, lassen wir unser Modell viele Epochen trainieren\n",
    "(`epochs = 500`). \n",
    "\n",
    "Am Ende kannst Du die *Lernkurve* des Modells sehen. **Wenn sie noch nach oben gehst, kannst Du das Modell weiter trainieren, indem Du die Zelle noch einmal aufrufst.**\n",
    "\n",
    "\n",
    "**Achtung:** Wenn Du die Zahl der Trainingsbilder erhöhst, solltest Du die Zahl der Epochen reduzieren, weil das Training sonst sehr lange dauert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 80\n",
    "\n",
    "with tqdm(range(epochs)) as iterator:\n",
    "\n",
    "    for epoch in iterator:\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            output = model(images)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            prediction = torch.argmax(output, 1)\n",
    "            train_acc += (prediction == labels).sum().item()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        training_acc.append(train_acc/len(train_set))\n",
    "        training_loss.append(train_loss/len(train_set))\n",
    "\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "                output = model(images)\n",
    "                loss = loss_fn(output, labels)\n",
    "                prediction = torch.argmax(output, 1)\n",
    "                test_acc += (prediction == labels).sum().item()\n",
    "                test_loss += loss.item()        \n",
    "            testing_acc.append(test_acc/len(test_set))\n",
    "            testing_loss.append(test_loss/len(test_set))\n",
    "\n",
    "        iterator.set_postfix_str(f\"train_acc: {train_acc/len(train_set):.2f} test_acc: {test_acc/len(test_set):.2f} train_loss: {train_loss/len(train_set):.2f} test_loss: {test_loss/len(test_set):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-rainbow",
   "metadata": {},
   "source": [
    "Die folgenden beiden Zellen geben die Lernkurven aus und zeigen die *Genauigkeit* und den *Verlust* jeweils für die Trainings- und Testdaten an.\n",
    "\n",
    "**Was fällt Dir beim Vergleich der Kurven für Test- und Trainingsadten auf?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(training_acc)), training_acc, label=\"train_acc\")\n",
    "plt.plot(range(len(training_acc)), testing_acc, label=\"test_acc\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), training_loss, label=\"train_loss\")\n",
    "plt.plot(range(epochs), testing_loss, label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78129882-b781-402f-8de1-080e5ea947bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
