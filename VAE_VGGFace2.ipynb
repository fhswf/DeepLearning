{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe11ae4-88f2-4a4c-89e9-9f1cdb370ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ProgramComputer/VGGFace2-HQ\", cache_dir=\"/home/shared-data/VGGFace2-HQ\", num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb69a656-2c4e-4b6f-a01c-3a968fed57ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_dataset in module datasets.load:\n",
      "\n",
      "load_dataset(path: str, name: Optional[str] = None, data_dir: Optional[str] = None, data_files: Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]], NoneType] = None, split: Union[str, datasets.splits.Split, NoneType] = None, cache_dir: Optional[str] = None, features: Optional[datasets.features.features.Features] = None, download_config: Optional[datasets.download.download_config.DownloadConfig] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None, verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None, ignore_verifications='deprecated', keep_in_memory: Optional[bool] = None, save_infos: bool = False, revision: Union[str, datasets.utils.version.Version, NoneType] = None, token: Union[bool, str, NoneType] = None, use_auth_token='deprecated', task='deprecated', streaming: bool = False, num_proc: Optional[int] = None, storage_options: Optional[Dict] = None, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]\n",
      "    Load a dataset from the Hugging Face Hub, or a local dataset.\n",
      "    \n",
      "    You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n",
      "    \n",
      "    A dataset is a directory that contains:\n",
      "    \n",
      "    - some data files in generic formats (JSON, CSV, Parquet, text, etc.).\n",
      "    - and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n",
      "    \n",
      "    Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n",
      "    \n",
      "    This function does the following under the hood:\n",
      "    \n",
      "        1. Download and import in the library the dataset script from `path` if it's not already cached inside the library.\n",
      "    \n",
      "            If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n",
      "    \n",
      "            Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\n",
      "            contain the path or URL to the original data files and the code to load examples from the original data files.\n",
      "    \n",
      "            You can find the complete list of datasets in the Datasets [Hub](https://huggingface.co/datasets).\n",
      "    \n",
      "        2. Run the dataset script which will:\n",
      "    \n",
      "            * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.\n",
      "            * Process and cache the dataset in typed Arrow tables for caching.\n",
      "    \n",
      "                Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
      "                They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
      "    \n",
      "        3. Return a dataset built from the requested splits in `split` (default: all).\n",
      "    \n",
      "    It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\n",
      "    In this case, it automatically loads all the data files from the directory or the dataset repository.\n",
      "    \n",
      "    Args:\n",
      "    \n",
      "        path (`str`):\n",
      "            Path or name of the dataset.\n",
      "            Depending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\n",
      "    \n",
      "            For local datasets:\n",
      "    \n",
      "            - if `path` is a local directory (containing data files only)\n",
      "              -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n",
      "              e.g. `'./path/to/directory/with/my/csv/data'`.\n",
      "            - if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n",
      "              -> load the dataset builder from the dataset script\n",
      "              e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\n",
      "    \n",
      "            For datasets on the Hugging Face Hub (list all available datasets with [`huggingface_hub.list_datasets`])\n",
      "    \n",
      "            - if `path` is a dataset repository on the HF hub (containing data files only)\n",
      "              -> load a generic dataset builder (csv, text etc.) based on the content of the repository\n",
      "              e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n",
      "            - if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n",
      "              -> load the dataset builder from the dataset script in the dataset repository\n",
      "              e.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n",
      "    \n",
      "        name (`str`, *optional*):\n",
      "            Defining the name of the dataset configuration.\n",
      "        data_dir (`str`, *optional*):\n",
      "            Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n",
      "            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
      "        data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
      "            Path(s) to source data file(s).\n",
      "        split (`Split` or `str`):\n",
      "            Which split of the data to load.\n",
      "            If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
      "            If given, will return a single Dataset.\n",
      "            Splits can be combined and specified like in tensorflow-datasets.\n",
      "        cache_dir (`str`, *optional*):\n",
      "            Directory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n",
      "        features (`Features`, *optional*):\n",
      "            Set the features type to use for this dataset.\n",
      "        download_config ([`DownloadConfig`], *optional*):\n",
      "            Specific download configuration parameters.\n",
      "        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "            Download/generate mode.\n",
      "        verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      "            Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "    \n",
      "            <Added version=\"2.9.1\"/>\n",
      "        ignore_verifications (`bool`, defaults to `False`):\n",
      "            Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "    \n",
      "            <Deprecated version=\"2.9.1\">\n",
      "    \n",
      "            `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\n",
      "            Please use `verification_mode` instead.\n",
      "    \n",
      "            </Deprecated>\n",
      "        keep_in_memory (`bool`, defaults to `None`):\n",
      "            Whether to copy the dataset in-memory. If `None`, the dataset\n",
      "            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
      "            nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n",
      "        save_infos (`bool`, defaults to `False`):\n",
      "            Save the dataset information (checksums/size/splits/...).\n",
      "        revision ([`Version`] or `str`, *optional*):\n",
      "            Version of the dataset script to load.\n",
      "            As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
      "            You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
      "        token (`str` or `bool`, *optional*):\n",
      "            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "        use_auth_token (`str` or `bool`, *optional*):\n",
      "            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "    \n",
      "            <Deprecated version=\"2.14.0\">\n",
      "    \n",
      "            `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n",
      "    \n",
      "            </Deprecated>\n",
      "        task (`str`):\n",
      "            The task to prepare the dataset for during training and evaluation. Casts the dataset's [`Features`] to standardized column names and types as detailed in `datasets.tasks`.\n",
      "    \n",
      "            <Deprecated version=\"2.13.0\">\n",
      "    \n",
      "            `task` was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
      "    \n",
      "            </Deprecated>\n",
      "        streaming (`bool`, defaults to `False`):\n",
      "            If set to `True`, don't download the data files. Instead, it streams the data progressively while\n",
      "            iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n",
      "    \n",
      "            Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
      "            Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
      "            like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
      "        num_proc (`int`, *optional*, defaults to `None`):\n",
      "            Number of processes when downloading and generating the dataset locally.\n",
      "            Multiprocessing is disabled by default.\n",
      "    \n",
      "            <Added version=\"2.7.0\"/>\n",
      "        storage_options (`dict`, *optional*, defaults to `None`):\n",
      "            **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n",
      "    \n",
      "            <Added version=\"2.11.0\"/>\n",
      "        **config_kwargs (additional keyword arguments):\n",
      "            Keyword arguments to be passed to the `BuilderConfig`\n",
      "            and used in the [`DatasetBuilder`].\n",
      "    \n",
      "    Returns:\n",
      "        [`Dataset`] or [`DatasetDict`]:\n",
      "        - if `split` is not `None`: the dataset requested,\n",
      "        - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n",
      "    \n",
      "        or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n",
      "    \n",
      "        - if `split` is not `None`, the dataset is requested\n",
      "        - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    Load a dataset from the Hugging Face Hub:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('rotten_tomatoes', split='train')\n",
      "    \n",
      "    # Map data files to splits\n",
      "    >>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
      "    ```\n",
      "    \n",
      "    Load a local dataset:\n",
      "    \n",
      "    ```py\n",
      "    # Load a CSV file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
      "    \n",
      "    # Load a JSON file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
      "    \n",
      "    # Load from a local loading script\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
      "    ```\n",
      "    \n",
      "    Load an [`~datasets.IterableDataset`]:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True)\n",
      "    ```\n",
      "    \n",
      "    Load an image dataset with the `ImageFolder` dataset builder:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4286997-8671-496a-9d0f-e9c8d03a88ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
