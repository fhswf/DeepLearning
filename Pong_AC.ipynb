{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b20f2cab-1ee5-4c0b-9d63-addece2efa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym[accept-rom-license,atari]\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (2.2.0)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting ale-py~=0.8.0\n",
      "  Using cached ale_py-0.8.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)\n",
      "Collecting autorom[accept-rom-license]~=0.4.2\n",
      "  Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\n",
      "Collecting AutoROM.accept-rom-license\n",
      "  Using cached AutoROM.accept_rom_license-0.4.2-py3-none-any.whl\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.3)\n",
      "Installing collected packages: gym-notices, gym, ale-py, AutoROM.accept-rom-license, autorom\n",
      "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.8.0 autorom-0.4.2 gym-0.26.2 gym-notices-0.0.8\n",
      "Collecting moviepy\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.10/site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.28.1)\n",
      "Collecting proglog<=1.0.0\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from moviepy) (1.22.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.21.3)\n",
      "Collecting imageio-ffmpeg>=0.2.0\n",
      "  Using cached imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.6.15.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
      "Installing collected packages: proglog, imageio-ffmpeg, decorator, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed decorator-4.4.2 imageio-ffmpeg-0.4.7 moviepy-1.0.3 proglog-0.1.10\n",
      "Collecting pysdl2\n",
      "  Using cached PySDL2-0.9.14-py3-none-any.whl\n",
      "Installing collected packages: pysdl2\n",
      "Successfully installed pysdl2-0.9.14\n",
      "Collecting pyvirtualdisplay\n",
      "  Using cached PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyvirtualdisplay\n",
      "Successfully installed pyvirtualdisplay-3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari,accept-rom-license]\n",
    "!pip install moviepy\n",
    "!pip install pysdl2\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc32e69c-6cd1-4fae-b0b0-8eb8bc0baef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.22.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afaac99b-c1f7-4561-beff-450a8dd57c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n",
      "/opt/conda/lib/python3.10/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/jovyan/work/Deep_Learning/video/pong-base folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "# record the game as as an mp4 file\n",
    "env = wrappers.RecordVideo(env, 'video/pong-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9077e2fd-1b30-4e5d-a71b-fa8f77823656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303ee66b-58f1-4436-9096-66901e1d6695",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob, info = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, truncated, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, truncated, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        super(EpisodicLifeEnv, self).__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "        self.was_real_reset = False\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs, info = self.env.reset()\n",
    "            self.was_real_reset = True\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _, info = self.env.step(0)\n",
    "            self.was_real_reset = False\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs, info\n",
    "    \n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, truncated, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs, info = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs, info\n",
    "\n",
    "    \n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        super(NoopResetEnv, self).__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset()\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = np.random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        info = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _, info = self.env.step(0)\n",
    "            if done:\n",
    "                obs, info = self.env.reset()\n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca7cd42-3672-4c19-9444-4f92515983a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env, stack_frames=True, episodic_life=True):\n",
    "    if episodic_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    #if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "    #    env = FireResetEnv(env)\n",
    "\n",
    "    env = WarpFrame(env)\n",
    "    if stack_frames:\n",
    "        env = FrameStack(env, 4)\n",
    "  \n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f601ecc-c1df-426d-8e6a-548ef60983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962e5704-6cd3-4d41-9480-7a7b662d9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env):\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0782329-cd11-4bab-bff2-dbc09240a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    " \n",
    "    def __init__(self, in_channels=4, n_actions=14):\n",
    "        \"\"\"\n",
    "        Initialize Actor Network\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_actions (int): number of outputs\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor_features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.critic_features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor_head = nn.Linear(256, n_actions)\n",
    "        self.critic_head = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255\n",
    "        x_a = self.actor_features(x)\n",
    "        x_c = self.critic_features(x)\n",
    "        return F.softmax(self.actor_head(x_a), dim=-1), self.critic_head(x_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b88156d3-935d-4f0f-a007-b8cd60c517f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', \n",
    "                        ('log_prob', 'value'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "026daa27-f114-4092-94b6-abf8a83a4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ActorCriticPolicy:\n",
    "    \n",
    "    GAMMA = 0.99\n",
    "    EPS = 1e-7\n",
    "    RENDER = False\n",
    "\n",
    "    \n",
    "    def __init__(self, lr=1e-4):\n",
    "        self.n_actions = 4\n",
    "        self.steps_done = 0\n",
    "        self.mean_reward = None\n",
    "        self.model = ActorCritic(n_actions=self.n_actions).to(device)\n",
    "        self.saved_actions = []\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.postfix = {}\n",
    "        \n",
    "    def get_state(self, obs):\n",
    "        state = np.array(obs)\n",
    "        state = state.transpose((2, 0, 1))\n",
    "        state = torch.from_numpy(state)\n",
    "        return state.unsqueeze(0).to(device)\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        state = self.get_state(observation)\n",
    "        probs, value = self.model(state)\n",
    "        \n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        self.saved_actions.append(SavedAction(m.log_prob(action), value))\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        return action.item()\n",
    "  \n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.state = self.get_state(observation)\n",
    "        self.total_reward = 0.0\n",
    "        self.rewards = []\n",
    "    \n",
    "    def update(self, obs, reward, terminated, truncated, info, pbar):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if not terminated:\n",
    "            self.next_state = self.get_state(obs)\n",
    "        else:\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "            self.postfix['total_reward'] = self.total_reward\n",
    "            self.postfix['mean_reward'] = self.mean_reward\n",
    "            self.postfix['steps'] = self.steps_done\n",
    "            pbar.set_postfix(self.postfix)\n",
    "            self.next_state = None\n",
    "            \n",
    "            self.finish_episode()    \n",
    "            \n",
    "        if self.steps_done % 100_000 == 0:\n",
    "            self.save(f'model_{self.steps_done}.pt')\n",
    "\n",
    "    def finish_episode(self):\n",
    "        R = 0\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        returns = []\n",
    "        \n",
    "        for r in self.rewards[::-1]:\n",
    "            if r != 0:\n",
    "                # Game boundary (Pong specific) !\n",
    "                R = 0\n",
    "            R = r + ActorCriticPolicy.GAMMA * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "        returns = torch.tensor(returns)\n",
    "        #returns = (returns - returns.mean()) / (returns.std() + ActorCriticPolicy.EPS)\n",
    "        \n",
    "        for (log_prob, value), R in zip(self.saved_actions, returns):\n",
    "            advantage = R - value.item()\n",
    "\n",
    "            # calculate actor (policy) loss\n",
    "            policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "            # calculate critic (value) loss using L1 smooth loss\n",
    "            value_losses.append(F.smooth_l1_loss(value, torch.tensor([R]).unsqueeze(0).to(device)))\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        # sum up all the values of policy_losses and value_losses\n",
    "        policy_loss = torch.stack(policy_losses).sum()\n",
    "        value_loss = torch.stack(value_losses).sum()\n",
    "        loss = policy_loss + value_loss\n",
    "        \n",
    "        self.postfix['policy_loss'] = policy_loss.item()\n",
    "        self.postfix['value_loss'] = value_loss.item()\n",
    "        self.postfix['loss'] = loss.item()\n",
    "\n",
    "        # perform backprop\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # reset rewards and action buffer\n",
    "        del self.rewards[:]\n",
    "        del self.saved_actions[:]\n",
    "\n",
    "        \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        print(checkpoint.keys())\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        if \"mean_reward\" in checkpoint:\n",
    "            self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        state = {\n",
    "                    'steps_done': self.steps_done,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward\n",
    "        }\n",
    "        print(state.keys())\n",
    "        torch.save(state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e63ce68-4b97-43dc-a817-8694357d2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ActorCriticPolicy(lr=1e-5)\n",
    "#policy.load(\"ac_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1783180-33be-459a-b839-d1bd100c5ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39e2e1c2fb747b4bbca7dcba6db9580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n",
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "policy.init_game(observation)\n",
    "\n",
    "plt.ion()\n",
    "plt.axis('off')\n",
    "img = plt.imshow(env.render())\n",
    "\n",
    "with tqdm(total=10000) as pbar:\n",
    "    while True:\n",
    "        try:\n",
    "            action = policy(observation)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            #render(env)\n",
    "            policy.update(observation, reward, terminated, truncated, info, pbar)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                pbar.update()\n",
    "                observation, info = env.reset()\n",
    "                policy.init_game(observation)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ecfeabc-4fdb-4371-8cde-e1c3a32e7e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['steps_done', 'model_state_dict', 'optimizer_state_dict', 'mean_reward'])\n"
     ]
    }
   ],
   "source": [
    "policy.save(\"ac_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4b6af-78ab-4edb-a449-31fe87aea7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
